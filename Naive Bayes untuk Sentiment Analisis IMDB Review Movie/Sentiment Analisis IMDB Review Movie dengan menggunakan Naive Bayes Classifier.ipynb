{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membaca file csv dengan library pandas\n",
    "data_positif = pd.read_csv('data training positif.csv')\n",
    "data_negatif = pd.read_csv('data training negatif.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = sentiment negatif\n",
    "1 = sentiment positif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6000\n",
       "0    6000\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#meng concate data positif dan negatif kedalam satu variable\n",
    "data = [data_positif,data_negatif]\n",
    "data_training = pd.concat(data)\n",
    "data_training.shape\n",
    "data_training['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x237424de388>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='class', data=data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Bromwell High is a cartoon comedy. It ran at t...\n",
       "1       I liked the film. Some of the action scenes we...\n",
       "2       Somewhat funny and well-paced action thriller ...\n",
       "3       Just two comments....SEVEN years apart? Hardly...\n",
       "4       Another Aussie masterpiece, this delves into t...\n",
       "                              ...                        \n",
       "5995    Having just recently re-viewed \"Lipstick\" for ...\n",
       "5996    It's pretty bad when the generic movie synopsi...\n",
       "5997    I saw 'Descent' last night at the Stockholm Fi...\n",
       "5998    Some films that you pick up for a pound turn o...\n",
       "5999    This is one of the dumbest films, I've ever se...\n",
       "Name: Text, Length: 12000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mengsplit / memisahkan menjadi per kata\n",
    "data_training['Text'] = data_training['Text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#menggunakan stopword untuk menghilangkan kata-kata yang tidak berpengaruh\n",
    "stop_list = ['his', 'didn', 'than', 'y', 'both', 'weren', 'from', 'again', 'have', 'having', 'her', 'are', \"isn't\", 'off', 'where', 'then', 'what', 'most', 'couldn', \"wasn't\", 'these', 'own', \"wouldn't\", 'was', 'can', 'a', 'above', 'if', 'haven', 'any', 'm', 'between', 'by', 'our', 'there', 'only', 'wasn', 'same', 'how', \"it's\", 'after', 'theirs', 'no', 'not', \"hadn't\", 'more', 'ain', 'few', 'been', 'did', \"won't\", \"should've\", \"mustn't\", 'just', \"don't\", 'into', \"hasn't\", 'themselves', 'who', 'you', 'when', 'up', 'yourselves', 'for', 'out', 'doesn', 'against', 'and', 'with', \"you've\", 'him', 'is', 'needn', 'we', 'under', 'll', 'down', 'mustn', 't', 'ma', 'why', 'yours', 'it', 'but', \"that'll\", \"she's\", 'each', 'now', 'i', \"needn't\", 'had', 'this', 'hadn', 'in', \"you'll\", 'during', 'its', 'the', 'through', 'to', \"you're\", 'do', 'too', 'd', 'aren', 'while', \"shan't\", 'all', 'of', 'being', 'very', 'at', 'because', 'will', \"mightn't\", 'those', 'doing', 'herself', 'should', 're', 'which', 'am', 'me', 'isn', 'wouldn', 'itself', 'before', 'as', 'does', \"aren't\", 'whom', 'here', 'shan', 'once', 'about', 'on', \"haven't\", 'yourself', 'an', 'don', 'he', 'o', 'she', 'further', 'their', 'ours', 'ourselves', 'be', 'below', 'myself', 'were', 'over', 'hasn', 'until', 'my', 'mightn', 'so', 'other', 've', 'that', \"couldn't\", \"weren't\", 'or', \"didn't\", 'nor', 'won', 'has', 's', 'shouldn', 'himself', 'hers', \"shouldn't\", 'they', 'them', \"you'd\", 'your', 'some', \"doesn't\", 'such']\n",
    "data_training['Text'] = data_training['Text'].apply(lambda x: [item for item in x if item not in stop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Bromwell, High, cartoon, comedy., It, ran, ti...\n",
       "1       [I, liked, film., Some, action, scenes, intere...\n",
       "2       [Somewhat, funny, well-paced, action, thriller...\n",
       "3       [Just, two, comments....SEVEN, years, apart?, ...\n",
       "4       [Another, Aussie, masterpiece,, delves, world,...\n",
       "                              ...                        \n",
       "5995    [Having, recently, re-viewed, \"Lipstick\", firs...\n",
       "5996    [It's, pretty, bad, generic, movie, synopsis, ...\n",
       "5997    [I, saw, 'Descent', last, night, Stockholm, Fi...\n",
       "5998    [Some, films, pick, pound, turn, rather, good,...\n",
       "5999    [This, one, dumbest, films,, I've, ever, seen....\n",
       "Name: Text, Length: 12000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kata yang sudah di pisahkan digabungkan kembali menjadi satu kesatuan\n",
    "data_training['Text'] = data_training['Text'].apply(lambda x: \",\".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kami akan membagi seluruh set data menjadi empat variabel; Train_X(atribut), Test_X(atribut), Train_Y(class), Test_Y(class), dengan rasio 7: 3 (train: test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data train :  8400\n",
      "Jumlah class train :  8400\n",
      "Jumlah data test :  3600\n",
      "Jumlah class test :  3600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels = data_training['class']\n",
    "text = data_training['Text']\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(text,labels,test_size=0.3,random_state=None)\n",
    "\n",
    "print(\"Jumlah data train : \", len(Train_X))\n",
    "print(\"Jumlah class train : \", len(Train_Y))\n",
    "print(\"Jumlah data test : \", len(Test_X))\n",
    "print(\"Jumlah class test : \", len(Test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    \n",
    "    #untuk menghilangkan angka, karakter dan hanya menyisakan huruf saja\n",
    "    text_bersih=re.sub('[^a-z\\s]+',' ',text,flags=re.IGNORECASE) \n",
    "    #untuk menghilangkan simbol break <br>\n",
    "    text_bersih=re.sub(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\",\" \",text_bersih)\n",
    "    #menghilangkan multiple space\n",
    "    text_bersih=re.sub('(\\s+)',' ',text_bersih)\n",
    "    #meng konvert semua kata menjadi lower case\n",
    "    text_bersih=text_bersih.lower()\n",
    "    \n",
    "    return text_bersih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rumus Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](naive-bayes-formula-1-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x : Data dengan class yang belum diketahui\n",
    "c : Hipotesis data merupakan suatu class spesifik\n",
    "P(c|x) : Probabilitas hipotesis berdasar kondisi (posteriori probability)\n",
    "P(c) : Probabilitas hipotesis (prior probability)\n",
    "P(x|c) : Probabilitas berdasarkan kondisi pada hipotesis\n",
    "P(x) : Probabilitas c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penggunaan Naive Bayes Pada Text Sentiment Analysis\n",
    "![title](hemeh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,unique_classes):\n",
    "        \n",
    "        self.classes=unique_classes #menyimpang unique class        \n",
    "\n",
    "    def BagOfWord(self,kata,index): #bertujuan untuk melakukan tokenized dan juga bag of word\n",
    "        \n",
    "        if isinstance(kata,np.ndarray): \n",
    "            kata=kata[0]\n",
    "     \n",
    "        for token_kata in kata.split(): #split seluruh kata\n",
    "          \n",
    "            self.bow_dicts[index][token_kata]+=1 #tambah satu ketika setiap kata yang sama pada dictionary\n",
    "            \n",
    "    def training_text(self,data_text,label_class): #untuk training dan juga melakukan processing model Naive Bayes\n",
    "    \n",
    "        self.examples=data_text #inisiasi untuk data text\n",
    "        self.labels=label_class #inisiasi untuk class\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])]) #membuat array sesuai dengan jumlah kelas\n",
    "        \n",
    "        #mengambil data bertipe data numpy arrays\n",
    "        \n",
    "        if not isinstance(self.examples,np.ndarray): \n",
    "            self.examples=np.array(self.examples)\n",
    "        if not isinstance(self.labels,np.ndarray): \n",
    "            self.labels=np.array(self.labels)\n",
    "            \n",
    "        #melakukan bag of word untuk setiap kelasnya\n",
    "        for index_kelas,kelas in enumerate(self.classes):\n",
    "          \n",
    "            text_kelas=self.examples[self.labels==kelas] #membagikan dokumen sesuai dengan kelas           \n",
    "           \n",
    "            #melakukan prepocessing data train\n",
    "            text_clean=[preprocessing_text(text) for text in text_kelas]\n",
    "            text_clean=pd.DataFrame(data=text_clean)\n",
    "            #melakukan bag of word untuk data train sesuai dengan kelasnya\n",
    "            np.apply_along_axis(self.BagOfWord,1,text_clean,index_kelas)\n",
    "            \n",
    "                \n",
    "\n",
    "        probab_kelas=np.zeros(self.classes.shape[0]) #membuat wadah untuk menyimpan hasil probabilitas untuk setiap kelasnya\n",
    "        seluruh_kata=[] #membuat list yang nanti akan diisikan kata kata sesuai dengan kelasnya\n",
    "        jumlah_kata=np.zeros(self.classes.shape[0]) #membuat array untuk menyimpan hasil perhitungan total kata tiap kelas\n",
    "        for index_kelas,kelas in enumerate(self.classes):\n",
    "           \n",
    "            #melakukan perhitungan untuk nilai prior probability p(c) untuk setiap kelasnya\n",
    "            probab_kelas[index_kelas]=np.sum(self.labels==kelas)/float(self.labels.shape[0]) \n",
    "            \n",
    "            #untuk menghitung semua kata pada setiap kelasnya\n",
    "            count=list(self.bow_dicts[index_kelas].values())\n",
    "            jumlah_kata[index_kelas]=np.sum(np.array(count))+1 # |v| is remaining to be added\n",
    "            \n",
    "            #menggabungkan semua kata dari setiap kelas                          \n",
    "            seluruh_kata+=self.bow_dicts[index_kelas].keys()\n",
    "                                                     \n",
    "    \n",
    "        #membuat sebuah vocabulary berisikan kata kata unique dari setiap kelasnya.\n",
    "        self.vocab=np.unique(np.array(seluruh_kata))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                  \n",
    "        #untuk mencari nilai pembagi dari rumus naive bayes                           \n",
    "        pembagi=np.array([jumlah_kata[index_kelas]+self.vocab_length+1 for kelas_index,kelas in enumerate(self.classes)])                                                                          \n",
    "        self.kelas_info=[(self.bow_dicts[kelas_index],probab_kelas[kelas_index],pembagi[kelas_index]) for kelas_index,kelas in enumerate(self.classes)]                               \n",
    "        self.kelas_info=np.array(self.kelas_info)                                 \n",
    "                                              \n",
    "                                              \n",
    "    def ProbabilitasDataTesting(self,data_test):                                                 \n",
    "                                              \n",
    "        likelihood_prob=np.zeros(self.classes.shape[0]) #membuat sebuah tempat untuk menyimpan nilai likelihood\n",
    "        \n",
    "        #mencari nilai probabilitas untuk setiap data test \n",
    "        for kelas_index,kelas in enumerate(self.classes): \n",
    "                             \n",
    "            for test_text in data_test.split(): #di split tiap kata di data test nya                          \n",
    "                \n",
    "                #menghitung jumlah kata pada data testing                          \n",
    "                test_token_counts=self.kelas_info[kelas_index][0].get(test_text,0)+1\n",
    "                \n",
    "                #menghitung nilai likelihood untuk data testing                              \n",
    "                test_token_prob=test_token_counts/float(self.kelas_info[kelas_index][2])                              \n",
    "                likelihood_prob[kelas_index]+=np.log(test_token_prob)\n",
    "                                              \n",
    "        # mencari nilai posterior probility\n",
    "        post_prob=np.empty(self.classes.shape[0])\n",
    "        for kelas_index,kelas in enumerate(self.classes):\n",
    "            post_prob[kelas_index]=likelihood_prob[kelas_index]+np.log(self.kelas_info[kelas_index][1])                                  \n",
    "      \n",
    "        return post_prob\n",
    "    \n",
    "   \n",
    "    def test(self,data_testing):\n",
    "      \n",
    "        hasil_prediksi=[] #menyimpan untuk hasil prediksi pada setiap data testing\n",
    "        for text in data_testing: \n",
    "                                              \n",
    "            #melakukan prepocessing pada data testing                                 \n",
    "            text_clean=preprocessing_text(text) \n",
    "             \n",
    "            #untuk mendapatkan nilai posterior probability pada setiap data testing untuk setiap kelasnya                               \n",
    "            post_prob=self.ProbabilitasDataTesting(text_clean)\n",
    "            \n",
    "            #lalu cari nilai maximum dari nilai posterior probability dari setiap kelasnya\n",
    "            hasil_prediksi.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(hasil_prediksi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=NaiveBayes(np.unique(Train_Y)) #melakukan proses unique pada label train\n",
    "nb.training_text(Train_X,Train_Y) #melakukan proses training pada data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nilai akurasi data Test: 86.194444 %\n"
     ]
    }
   ],
   "source": [
    "#untuk menghasilkan nilai prediksi dari data test\n",
    "pclasses=nb.test(Test_X)\n",
    "nilai_akurasi=(np.sum(pclasses==Test_Y)/float(Test_Y.shape[0]) * 100)\n",
    "print (\"Nilai akurasi data Test: {:2f} %\".format(nilai_akurasi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
